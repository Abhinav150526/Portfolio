Abhinav Singarapu - Senior Data Engineer

513-318-8292        singarapu1999@gmail.com	 Fairborn, Ohio

Professional Summary:

Around 6 years of IT experience with expertise in big data technologies like Hadoop, Spark, Kafka, Hive, and HDFS, including real-time data processing using Spark Streaming and Kafka integration.

Proficient in designing and optimizing scalable data engineering pipelines, leveraging Spark RDDs, DataFrames, SQL, and tools such as EMR, Redshift, S3, Glue, Databricks, and Snowflake for advanced data transformation and storage.

Skilled in Python, PySpark, and Scala for data preprocessing and processing large datasets, alongside experience in data governance, quality checks, and robust ETL pipelines using Airflow, Git, and Bitbucket.

Expertise in cloud platforms such as AWS (S3, Redshift, Lambda, Glue) and Azure (ADF, Synapse, Data Lake) to deliver comprehensive data solutions and implement scalable architectures, including infrastructure automation with Terraform.

Proficient in developing interactive dashboards and reports using Power BI and optimizing SQL queries for relational databases like Oracle, MySQL, and PostgreSQL, as well as NoSQL systems like DynamoDB and Cassandra.

Experienced in implementing CI/CD pipelines with Git, Bitbucket, and Jenkins, and in using Jira for agile project management to streamline development cycles and deploy high-quality, data-driven solutions effectively.

Technical Skills:

Professional Experience:

Verizon, Cincinnati, OH.	            Jan 2024 - Present

Senior Data Engineer

Responsibilities:

Implemented automated testing frameworks and tools to ensure high-quality software releases. Employed Test-Driven Development (TDD) practices to align tests with business requirements.

Designed and Implemented Hadoop-based data architectures to store and process large datasets, enhancing data accessibility and analytics capabilities across the organization.

Developed scalable ETL processes using Apache Hadoop, leveraging Spark and Hive for data transformation and analysis of structured and unstructured data.

Developed and Optimized data processing applications using Apache Spark and Scala, significantly improving data processing times and handling large-scale datasets in distributed environments.

Developed and maintained robust data integration pipelines using tools like Apache Airflow, AWS Glue and Step functions ensuring seamless data flow between multiple systems.

Built end-to-end data pipelines for ingesting, transforming, and managing structured, semi-structured, and unstructured data within a unified Lakehouse architecture.

Integrated data from disparate sources, such as SQL, NoSQL, APIs, and cloud platforms, enabling unified analytics and reporting solutions for business stakeholders.

Implemented PySpark Streaming solutions for real-time data processing, enabling the ingestion and analysis of streaming data from various sources such as Kafka and Flume.

Used Agile tools and techniques to track progress, manage risks, and ensure timely delivery of high-quality products.

Uber, Dallas, TX.	            May 2023 - Dec 2023

Cloud Data Engineer

Responsibilities:

Worked on a project to transform raw business data using SQL Server, writing complex SQL queries for data cleansing, aggregation, and transformation.

Designed and executed advanced SQL scripts to extract and transform data, addressing business requirements by implementing joins, subqueries, and case statements for customized data sets.

Developed data pipelines using AWS Glue, Spark, and Hive to ingest and process customer data, ensuring scalability and efficiciency.

Converted Hive queries into Spark transformations using AWS Glue and PySpark, improving performance and reducing processing time.

Strong proficiency in AWS RDS (PostgreSQL/Oracle) and Aurora, including writing complex SQL queries, stored procedures, and triggers.

Implemented security measures in AWS RDS using IAM roles, encryption, and VPC-based access control to restrict unauthorized access to sensitive data.

Experienced in creating conceptual, logical, and physical data models using Erwin, aligning with AWS Redshift and other AWS database services.

Integrated AWS services like S3, Lambda, and ECS with DevOps workflows to streamline application deployments and enhance operational efficiency.

Deep understanding of Kubernetes infrastructure and AWS, ensuring seamless integration, scaling, and management of distributed applications within containerized environments.

Documented ETL workflows and transformation logic, ensuring clarity and reproducibility for data processes from SQL Server to Power BI dashboards.

Developed automated testing frameworks in alignment with Test-Driven Development (TDD) practices to validate data integrity for SQL and Power BI reporting solutions.

Created Power BI dashboards to visualize sales, operational trends, and performance metrics, integrating transformed data from SQL Server.

Structured data models in Power BI by leveraging transformed datasets from SQL queries, ensuring efficient performance and user-friendly visualizations.

Implemented role-based access control using SQL transformations and Power BI Row-Level Security (RLS), restricting dashboard views based on user roles.

Data Engineer

Citi, Hyderabad                                                                                                                     April 2021 - July 2022

Responsibilities:

Designed and implemented Azure-based data architectures to integrate, store, and process large datasets from SQL and Oracle databases, enhancing accessibility and analytics capabilities across the organization.

Developed and optimized ETL pipelines using Azure Data Factory to transform and transfer structured and semi-structured data from SQL sources into Azure Data Lake and Power BI for reporting.

Built scalable data processing workflows on Azure Synapse Analytics, enabling efficient analysis of large datasets and improving query performance for business intelligence applications.

Developed automated testing frameworks in alignment with Test-Driven Development (TDD) practices to validate data integrity for SQL and Power BI reporting solutions.

Integrated data from Oracle databases, APIs, and cloud platforms into Azure Data Lake, creating unified datasets for seamless analytics and enhanced decision-making.

Implemented advanced SQL queries and performance tuning techniques to ensure optimal performance of data processing pipelines and reporting workflows in Power BI.

Designed robust Power BI dashboards for real-time data visualization, combining data from SQL Server, Azure Blob Storage, and Oracle databases, meeting business stakeholder requirements.

Designed and implemented semantic data models to create a unified layer for business reporting, enabling consistent and accurate analysis across enterprise data sources, including Oracle databases.

Developed advanced semantic modeling for OLAP-based analytics, optimizing Oracle data structures to improve query performance and ensure scalability for large datasets.

Developed end-to-end ETL workflows in Azure, ensuring seamless ingestion and transformation of data from on-premises Oracle systems to the cloud environment.

Automated reporting pipelines in Power BI by integrating dynamic datasets from SQL and Azure Synapse Analytics, providing actionable insights and improving data visibility.

Streamlined data processing workflows by utilizing Azure Functions for event-driven automation, reducing manual interventions in integrating SQL and Oracle data sources.

Implemented and maintained governance policies for Azure Data Lake and Power BI reporting, ensuring compliance with data security standards and organizational requirements.

Used Git for version control, fostering collaboration and maintaining code quality.

Used Agile tools and techniques to track progress, manage risks, and ensure timely delivery of high-quality products.

Analyst

Cyient, Hyderabad                                                                                                             April 2018 - March 2021

Responsibilities:

Performed data cleaning and preparation by identifying and resolving inconsistencies, missing values, and duplicates using SQL, Python, and Excel, ensuring high data quality for analysis and reporting.

Performed data cleaning and preparation by identifying and resolving inconsistencies, missing values, and duplicates using SQL, Python, and Excel, ensuring high data quality for analysis and reporting.

Developed interactive dashboards using Power BI and Tableau to visualize key business metrics such as sales performance and customer behavior, enabling stakeholders to make data-driven decisions.

Implemented row-level security (RLS) in Power BI to restrict data access based on user roles, ensuring compliance with organizational security policies.

Automated the refresh of datasets in Power BI Service using scheduled gateways, ensuring up-to-date reporting for end-users.

Conducted exploratory data analysis (EDA) to identify trends, patterns, and anomalies in sales and operational data, delivering actionable insights that supported strategic business initiatives.

Collaborated with cross-functional teams to gather requirements and deliver customized reports and visualizations, aligning analytics outputs with business objectives and improving decision-making efficiency.

Maintained comprehensive documentation of data workflows, transformations, and reporting processes, ensuring transparency and reproducibility for future analytics initiatives.

Education:

Master of Science (M.S.) in Computer Science         Wright State University, Faiborn, OH, US

Bachelor of Technology (B.Tech) in Computer Science Jawaharlal Nehru Technological University, Hyderabad

